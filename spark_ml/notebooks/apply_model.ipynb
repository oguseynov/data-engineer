{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StringType}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current spark version is 2.4.4\n"
     ]
    }
   ],
   "source": [
    "println(s\"Current spark version is ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputStreamPath = /home/jovyan/data/events-stream\n",
       "modelPath = /home/jovyan/models/spark-ml-model\n",
       "dataSchema = StructType(StructField(tweet,StringType,true))\n",
       "inputDF = [tweet: string, timestamp: timestamp]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tweet: string, timestamp: timestamp]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputStreamPath = \"/home/jovyan/data/events-stream\"\n",
    "val modelPath = \"/home/jovyan/models/spark-ml-model\"\n",
    "\n",
    "val dataSchema = new StructType()\n",
    "    .add(\"tweet\", StringType)\n",
    "\n",
    "val inputDF = spark\n",
    "    .readStream\n",
    "    .schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(inputStreamPath)\n",
    "    .withColumn(\"timestamp\", lit(current_timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = pipeline_3dfeba337f5a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_3dfeba337f5a"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = PipelineModel.load(\"/home/jovyan/models/spark-ml-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsDF = [tweet: string, timestamp: timestamp ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tweet: string, timestamp: timestamp ... 6 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsDF = model.transform(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getProbability = UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val getProbability = udf((prediction: org.apache.spark.ml.linalg.Vector) => prediction(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:39: error: stable identifier required, but this.$line7$read.spark.implicits found.\n",
       "       import spark.implicits._\n",
       "                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7c45144f"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  clean_probability|\n",
      "+-------------------+\n",
      "|  0.506119085226794|\n",
      "|0.44409918165014944|\n",
      "|0.49065237891017643|\n",
      "| 0.5115763550216391|\n",
      "| 0.5021846446881393|\n",
      "|0.48400688875371084|\n",
      "|  0.506119085226794|\n",
      "| 0.5095230413582319|\n",
      "| 0.4984422398503117|\n",
      "| 0.5115763550216391|\n",
      "| 0.5594890772238319|\n",
      "| 0.5115763550216391|\n",
      "|0.44409918165014944|\n",
      "| 0.5115763550216391|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n",
    "    batchDF.persist()\n",
    "    \n",
    "    val cleanProbability = batchDF.select(getProbability($\"probability\").alias(\"clean_probability\"))\n",
    "    cleanProbability.write.mode(\"append\").format(\"console\").save()\n",
    "    \n",
    "    val windowCount = batchDF\n",
    "        .withWatermark(\"timestamp\", \"20 seconds\")\n",
    "        .groupBy(window($\"timestamp\", \"10 seconds\", \"1 seconds\"), $\"prediction\")\n",
    "        .count()\n",
    "    windowCount.write.mode(\"append\").format(\"console\").save()\n",
    "    \n",
    "    batchDF.unpersist()\n",
    "}.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
